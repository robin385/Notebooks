{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "\n",
    "data=pd.read_csv('train.csv')\n",
    "data['id'] = data['id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing video data\n",
    "processing video that goes in the DashCam\n",
    "- Event type - Whether the video contains collision/near-collision or normal driving;\n",
    "- Event time - Time at which a (near-)collision happens (if appliable);\n",
    "- Alert time â€“ The earliest time in the video when the accident (or near-collision) could be predicted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _pad_or_sample_clip(frames, target_len):\n",
    "    \"\"\"Ensure the list of frames has length == target_len by sampling or padding.\"\"\"\n",
    "    n = len(frames)\n",
    "    if n == target_len:\n",
    "        return frames\n",
    "    if n < target_len:\n",
    "        # Pad by repeating the last frame\n",
    "        last_frame = frames[-1]\n",
    "        frames.extend([last_frame] * (target_len - n))\n",
    "        return frames\n",
    "    else:\n",
    "        # Sample uniformly to reduce to target_len\n",
    "        idxs = np.linspace(0, n - 1, num=target_len, dtype=int)\n",
    "        return [frames[i] for i in idxs]\n",
    "\n",
    "def get_event_clip(frames, fps, t_alert, t_event, clip_length_sec=8, fixed_frames=16):\n",
    "    \"\"\"\n",
    "    Extract a clip from the list of frames based on event timing,\n",
    "    then downsample (if needed) and finally adjust the clip to a fixed number of frames.\n",
    "    \"\"\"\n",
    "    total_frames = len(frames)\n",
    "    if t_event is None:\n",
    "        end_idx = total_frames - 1\n",
    "    else:\n",
    "        end_idx = min(total_frames - 1, int(t_event * fps))\n",
    "    start_idx = max(0, end_idx - int(clip_length_sec * fps))\n",
    "    clip = frames[start_idx: end_idx + 1]\n",
    "    \n",
    "    return np.array(clip)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data augmentation \n",
    "- randomized cropping\n",
    "- random horizontal flipping\n",
    "- random b/con\n",
    "- Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Define an augmentation pipeline for a single frame\n",
    "aug = A.Compose([\n",
    "    #A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0)),\n",
    "    A.Resize(242, 430),#430\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.MotionBlur(blur_limit=3, p=0.2),\n",
    "    # ... add more as needed\n",
    "    A.Normalize(mean=(0.45,0.45,0.45), std=(0.225,0.225,0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "def augment_clip(frames):\n",
    "    \"\"\"\n",
    "    Applies the same augmentation to each frame in a list and\n",
    "    returns a torch tensor of shape (T, C, H, W).\n",
    "    \"\"\"\n",
    "    augmented = []\n",
    "    for frame in frames:\n",
    "        # albumentations expects images in HxWxC order (uint8)\n",
    "        image_aug = aug(image=frame)['image']\n",
    "        augmented.append(image_aug)\n",
    "    return torch.stack(augmented)  # shape: (T, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the output video parameters\n",
    "def save_video(clip):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = 24  # Adjust as needed\n",
    "    frame_size = (355, 200)  # Should match the resized frame dimensions\n",
    "\n",
    "    # Create the VideoWriter object\n",
    "    out = cv2.VideoWriter('train/000000000000.mp4', fourcc, fps, frame_size)\n",
    "\n",
    "    # Write each frame to the output video file\n",
    "    for frame in clip:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting distribution\n",
    "- length of video clips and normalizing entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean clip length: 1.6003573333333334\n",
      "Sample random clip length from distribution: 1.6690000000000005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "clip_lengths = []\n",
    "# Loop over each video in the DataFrame\n",
    "for index, video in data.iterrows():\n",
    "    if not math.isnan(video['time_of_event']):\n",
    "        t_event = video['time_of_event']\n",
    "        t_alert = video['time_of_alert']\n",
    "        # Calculate clip length in seconds from t_event and t_alert\n",
    "        clip_length_sec = t_event - t_alert\n",
    "\n",
    "        clip_lengths.append(clip_length_sec)\n",
    "        \n",
    "\n",
    "# Plot the distribution of clip lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(clip_lengths, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution of Clip Lengths\")\n",
    "plt.xlabel(\"Clip Length (frames)\")\n",
    "plt.ylabel(\"Number of Clips\")\n",
    "plt.show()\n",
    "print(\"Mean clip length:\", np.mean(clip_lengths))\n",
    "\n",
    "\n",
    "# Define a function to return a random clip length from the observed distribution\n",
    "def get_random_length():\n",
    "    return random.choice(clip_lengths)\n",
    "\n",
    "print(\"Sample random clip length from distribution:\", get_random_length())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollisionDataset(Dataset):\n",
    "    def __init__(self, df, video_dir, augment=False):\n",
    "        \"\"\"\n",
    "        Initialize the CollisionDataset.\n",
    "        \n",
    "        For collision videos (with valid time_of_alert and time_of_event), this class:\n",
    "          - Extracts a singular clip spanning from time_of_alert to time_of_event.\n",
    "          - Generates a target label as a scalar 1 (collision).\n",
    "        \n",
    "        For non-collision videos, a single random clip is extracted (ensuring at least self.clip_length frames)\n",
    "        and its label is 0 (no collision).\n",
    "        \n",
    "        Parameters:\n",
    "          df: Pandas DataFrame with video metadata. Must contain:\n",
    "              - 'id': video identifier (will be zero-filled to length 5 and appended with '.mp4')\n",
    "              - 'time_of_alert' and 'time_of_event' for collision videos.\n",
    "          video_dir: Directory where video files are stored.\n",
    "          augment: Whether to apply augmentation on clips.\n",
    "        \"\"\"\n",
    "        self.video_dir = video_dir\n",
    "        self.augment = augment\n",
    "        self.clip_length = 16  # fixed number of frames per clip\n",
    "        self.segments = []     # list of tuples: (video_path, start_frame, end_frame, label)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Build video filename using the 'id' column\n",
    "            video_filename = str(row['id']).zfill(5) + \".mp4\"\n",
    "            video_path = (video_filename if os.path.isabs(video_filename)\n",
    "                          or video_dir == \"\" else os.path.join(video_dir, video_filename))\n",
    "            \n",
    "            # Open video to get total frame count and FPS\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Skipping {video_path}: cannot be opened\")\n",
    "                continue\n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            if fps is None or fps <= 0:\n",
    "                fps = 30.0  # fallback FPS\n",
    "            cap.release()\n",
    "            \n",
    "            # Determine if this is a collision video (requires time_of_alert and time_of_event)\n",
    "            if ('time_of_alert' in row and row['time_of_alert'] not in [None, \"\", np.nan] and pd.notna(row['time_of_alert'])) and \\\n",
    "               ('time_of_event' in row and row['time_of_event'] not in [None, \"\", np.nan] and pd.notna(row['time_of_event'])):\n",
    "                t_alert = float(row['time_of_alert'])\n",
    "                t_event = float(row['time_of_event'])\n",
    "                start_frame = int(t_alert * fps)\n",
    "                event_frame = int(t_event * fps)\n",
    "                if event_frame >= total_frames:\n",
    "                    event_frame = total_frames - 1\n",
    "                end_frame = event_frame + 1  # exclusive\n",
    "                # Instead of a temporal vector, set the label to 1 (collision)\n",
    "                label = np.array(1, dtype=np.float32)\n",
    "                self.segments.append((video_path, start_frame, end_frame, label))\n",
    "            else:\n",
    "                # Non-collision video: extract 4 random clips with label 0\n",
    "                if total_frames < self.clip_length:\n",
    "                    print(f\"Skipping {video_path}: too short\")\n",
    "                    continue\n",
    "                prev_end_frame = 0\n",
    "                for _ in range(3):  # select 3 random clips (or adjust the number as needed)\n",
    "                    start_frame = prev_end_frame\n",
    "                    if start_frame >= total_frames:\n",
    "                        break\n",
    "                    # Use your get_random_length() function to select a clip length\n",
    "                    clip_len = get_random_length()\n",
    "                    end_frame = start_frame + clip_len\n",
    "                    label = np.array(0, dtype=np.float32)\n",
    "                    self.segments.append((video_path, start_frame, end_frame, label))\n",
    "                    prev_end_frame = end_frame + 2\n",
    "                            \n",
    "    def __len__(self):\n",
    "        return len(self.segments)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path, start_frame, end_frame, label = self.segments[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise IOError(f\"Cannot open video: {video_path}\")\n",
    "            \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        frames = []\n",
    "        frame_idx = start_frame\n",
    "\n",
    "        while frame_idx < end_frame:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            # Resize frame; note: cv2.resize uses (width, height)\n",
    "            frames.append(frame)\n",
    "            frame_idx += 1\n",
    "            \n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            raise RuntimeError(f\"No frames read for segment {idx} from video {video_path}\")\n",
    "\n",
    "        # Ensure the clip has self.clip_length frames\n",
    "        frames = _pad_or_sample_clip(frames, self.clip_length)\n",
    "\n",
    "        if self.augment:\n",
    "            # Augment the clip; resulting tensor will be (T, C, H, W)\n",
    "            clip_tensor = augment_clip(frames)\n",
    "            clip_tensor = clip_tensor.permute(1, 0, 2, 3)  # rearrange to (C, T, H, W)\n",
    "        else:\n",
    "            # Convert frames to a tensor; from (T, H, W, C) to (C, T, H, W)\n",
    "            clip_array = np.stack(frames, axis=0).astype(np.float32) / 255.0\n",
    "            clip_array = np.transpose(clip_array, (3, 0, 1, 2))\n",
    "            clip_tensor = torch.from_numpy(clip_array)\n",
    "\n",
    "        # Convert label to a torch tensor (scalar)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return clip_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### finally at the training loop\n",
    "- Resnet approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c2d_r50', 'csn_r101', 'efficient_x3d_s', 'efficient_x3d_xs', 'i3d_r50', 'mvit_base_16', 'mvit_base_16x4', 'mvit_base_32x3', 'r2plus1d_r50', 'slow_r50', 'slow_r50_detection', 'slowfast_16x8_r101_50_50', 'slowfast_r101', 'slowfast_r50', 'slowfast_r50_detection', 'x3d_l', 'x3d_m', 'x3d_s', 'x3d_xs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 - Batch 5/3000 (0.2% done) - Batch Loss: 0.6334 - Est. Time Left: 2612.10s\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[117]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m labels = labels.to(device).unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclips\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Expected output shape: (B, 1)\u001b[39;00m\n\u001b[32m     54\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m     55\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/models/vision_transformers.py:179\u001b[39m, in \u001b[36mMultiscaleVisionTransformers.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    177\u001b[39m thw = \u001b[38;5;28mself\u001b[39m.cls_positional_encoding.patch_embed_shape()\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     x, thw = \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m x = \u001b[38;5;28mself\u001b[39m.norm_embed(x)\n\u001b[32m    181\u001b[39m x = \u001b[38;5;28mself\u001b[39m.head(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/layers/attention.py:743\u001b[39m, in \u001b[36mMultiScaleBlock.forward\u001b[39m\u001b[34m(self, x, thw_shape)\u001b[39m\n\u001b[32m    732\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    733\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    734\u001b[39m \u001b[33;03m    x (torch.Tensor): Input tensor.\u001b[39;00m\n\u001b[32m    735\u001b[39m \u001b[33;03m    thw_shape (List): The shape of the input tensor (before flattening).\u001b[39;00m\n\u001b[32m    736\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    738\u001b[39m x_norm = (\n\u001b[32m    739\u001b[39m     \u001b[38;5;28mself\u001b[39m.norm1(x.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)).permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    740\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm1_is_batchnorm_1d\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm1(x)\n\u001b[32m    742\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m743\u001b[39m x_block, thw_shape_new = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthw_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dim_mul_in_att \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dim != \u001b[38;5;28mself\u001b[39m.dim_out:\n\u001b[32m    745\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.proj(x_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/layers/attention.py:529\u001b[39m, in \u001b[36mMultiScaleAttention.forward\u001b[39m\u001b[34m(self, x, thw_shape)\u001b[39m\n\u001b[32m    523\u001b[39m         qkv = (\n\u001b[32m    524\u001b[39m             \u001b[38;5;28mself\u001b[39m.qkv(x)\n\u001b[32m    525\u001b[39m             .reshape(B, N, \u001b[32m3\u001b[39m, \u001b[38;5;28mself\u001b[39m.num_heads, -\u001b[32m1\u001b[39m)\n\u001b[32m    526\u001b[39m             .permute(\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m4\u001b[39m)\n\u001b[32m    527\u001b[39m         )\n\u001b[32m    528\u001b[39m         q, k, v = qkv[\u001b[32m0\u001b[39m], qkv[\u001b[32m1\u001b[39m], qkv[\u001b[32m2\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m     q, q_shape, k, k_shape, v, v_shape = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_qkv_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthw_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    531\u001b[39m attn = (q * \u001b[38;5;28mself\u001b[39m.scale) @ k.transpose(-\u001b[32m2\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m    532\u001b[39m attn = attn.softmax(dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/layers/attention.py:463\u001b[39m, in \u001b[36mMultiScaleAttention._qkv_pool\u001b[39m\u001b[34m(self, q, k, v, thw_shape)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_qkv_pool\u001b[39m(\n\u001b[32m    454\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    455\u001b[39m     q: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    460\u001b[39m     torch.Tensor, List[\u001b[38;5;28mint\u001b[39m], torch.Tensor, List[\u001b[38;5;28mint\u001b[39m], torch.Tensor, List[\u001b[38;5;28mint\u001b[39m]\n\u001b[32m    461\u001b[39m ]:\n\u001b[32m    462\u001b[39m     q, q_shape = \u001b[38;5;28mself\u001b[39m._attention_pool_q(q, thw_shape)\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     k, k_shape = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_attention_pool_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthw_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m     v, v_shape = \u001b[38;5;28mself\u001b[39m._attention_pool_v(v, thw_shape)\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m q, q_shape, k, k_shape, v, v_shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/torch/hub/facebookresearch_pytorchvideo_main/pytorchvideo/layers/attention.py:189\u001b[39m, in \u001b[36m_AttentionPool.forward\u001b[39m\u001b[34m(self, tensor, thw_shape)\u001b[39m\n\u001b[32m    187\u001b[39m B, N, L, C = tensor.shape\n\u001b[32m    188\u001b[39m T, H, W = thw_shape\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m tensor = \u001b[43mtensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m)\u001b[49m.permute(\u001b[32m0\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m).contiguous()\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm_before_pool:\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# If use BN, we apply norm before pooling instead of after pooling.\u001b[39;00m\n\u001b[32m    193\u001b[39m     tensor = \u001b[38;5;28mself\u001b[39m.norm(tensor)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "# make sure you have the appropriate collision dataset defined as CollisionDataset\n",
    "print(torch.hub.list('facebookresearch/pytorchvideo'))\n",
    "\n",
    "# --- Prepare your dataset and dataloader ---\n",
    "video_dir = \"train\"  # directory where your videos are stored\n",
    "dataset = CollisionDataset(df=data, video_dir=video_dir, augment=True)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "# --- Define the model using MViTv2 ---\n",
    "# Load a MViTv2 model from PyTorchVideo. Here we load the \"small\" variant.\n",
    "model = torch.hub.load('facebookresearch/pytorchvideo', 'mvit_base_32x3', pretrained=True)\n",
    "\n",
    "# Replace the classification head to output a single logit for binary classification.\n",
    "# The MViTv2 model typically has a head with a projection, so we reassign that.\n",
    "model.head.proj = nn.Linear(model.head.proj.in_features, 1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Define loss and optimizer ---\n",
    "# Here we use BCEWithLogitsLoss; adjust pos_weight if necessary.\n",
    "pos_weight = torch.tensor(0.5).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# --- Training loop with progress printing ---\n",
    "num_epochs = 7\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_start_time = time.time()  # Start time of the epoch\n",
    "    epoch_loss = 0.0\n",
    "    total_batches = len(dataloader)\n",
    "    \n",
    "    for batch_idx, (clips, labels) in enumerate(dataloader, start=1):\n",
    "        # Move data to device\n",
    "        clips = clips.float().to(device)\n",
    "        # Ensure labels have shape (B, 1) to match outputs\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(clips)  # Expected output shape: (B, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item() * clips.size(0)\n",
    "        \n",
    "        # Progress printing\n",
    "        progress = 100 * batch_idx / total_batches\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        avg_batch_time = elapsed / batch_idx\n",
    "        estimated_remaining = avg_batch_time * (total_batches - batch_idx)\n",
    "        \n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} - Batch {batch_idx}/{total_batches} \"\n",
    "            f\"({progress:.1f}% done) - Batch Loss: {loss.item():.4f} - \"\n",
    "            f\"Est. Time Left: {estimated_remaining:.2f}s\",\n",
    "            end=\"\\r\"\n",
    "        )\n",
    "    \n",
    "    epoch_loss /= len(dataset)\n",
    "    epoch_duration = time.time() - epoch_start_time\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} finished, Average Loss: {epoch_loss:.4f}, Epoch Time: {epoch_duration:.2f}s\\n\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model, \"collision_model_MViTv2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_dir = \"train\"  # directory where your videos are stored\n",
    "# dataset = CollisionDataset(dataframe=data, video_dir=\"train\", fps=30, transform=True)\n",
    "# print('Number of samples:', dataset.__len__())\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=8,\n",
    "#     shuffle=True,\n",
    "#     num_workers=8  # You can increase this to 2 or 4 if not on Windows\n",
    "# )\n",
    "\n",
    "# # --- Define the model ---\n",
    "# model = models.video.r3d_18(weights=models.video.R3D_18_Weights.DEFAULT)\n",
    "# model.fc = nn.Linear(model.fc.in_features, 1)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# # --- Define loss and optimizer ---\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# # --- Training loop with progress printing ---\n",
    "# num_epochs = 11\n",
    "\n",
    "# model.train()\n",
    "# import time\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     epoch_start_time = time.time()  # Start time of the epoch\n",
    "#     epoch_loss = 0.0\n",
    "#     total_batches = len(dataloader)\n",
    "    \n",
    "#     for batch_idx, (clips, labels) in enumerate(dataloader, start=1):\n",
    "#         clips = clips.float().to(device)\n",
    "#         labels = labels.unsqueeze(1).to(device)  # shape: (B, 1)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(clips)  # outputs shape: (B, 1)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item() * clips.size(0)\n",
    "        \n",
    "#         # Calculate progress and estimated time remaining\n",
    "#         progress = 100 * batch_idx / total_batches\n",
    "#         elapsed = time.time() - epoch_start_time\n",
    "#         avg_batch_time = elapsed / batch_idx\n",
    "#         estimated_remaining = avg_batch_time * (total_batches - batch_idx)\n",
    "        \n",
    "#         print(\n",
    "#             f\"Epoch {epoch+1}/{num_epochs} - Batch {batch_idx}/{total_batches} \"\n",
    "#             f\"({progress:.1f}% done) - Batch Loss: {loss.item():.4f} - \"\n",
    "#             f\"Est. Time Left: {estimated_remaining:.2f}s\",\n",
    "#             end=\"\\r\"\n",
    "#         )\n",
    "    \n",
    "#     epoch_loss /= len(dataset)\n",
    "#     epoch_duration = time.time() - epoch_start_time\n",
    "#     print(f\"\\nEpoch {epoch+1}/{num_epochs} finished, Average Loss: {epoch_loss:.4f}, Epoch Time: {epoch_duration:.2f}s\\n\")\n",
    "\n",
    "# torch.save(model, \"collision_model.pth\")\n",
    "\n",
    "#662 min 12 epoch 375 batch 20 min 375\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision.models as models\n",
    "import pytorchvideo\n",
    "import torch.nn as nn\n",
    "\n",
    "# --- Define a test transform (deterministic normalization and tensor conversion) ---\n",
    "test_aug = A.Compose([\n",
    "    #A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0)),\n",
    "    A.Resize(242, 430),#430\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.3),\n",
    "    A.MotionBlur(blur_limit=3, p=0.2),\n",
    "    # ... add more as needed\n",
    "    A.Normalize(mean=(0.45,0.45,0.45), std=(0.225,0.225,0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "def test_transform_clip(frames):\n",
    "    \"\"\"\n",
    "    Applies a deterministic transform (normalization and conversion to tensor)\n",
    "    to a list of frames.\n",
    "    Returns a tensor of shape (T, C, H, W).\n",
    "    \"\"\"\n",
    "    transformed_frames = []\n",
    "    for frame in frames:\n",
    "        # Albumentations expects HxWxC image\n",
    "        transformed = test_aug(image=frame)['image']\n",
    "        transformed_frames.append(transformed)\n",
    "    return torch.stack(transformed_frames)\n",
    "\n",
    "def sliding_window_clips(frames, window_size, stride):\n",
    "    \"\"\"\n",
    "    Splits the full list of frames into clips using a sliding window.\n",
    "    Returns a list of clips (each a list of frames) and their corresponding start indices.\n",
    "    \"\"\"\n",
    "    clips = []\n",
    "    indices = []\n",
    "    for start in range(0, len(frames) - window_size + 1, stride):\n",
    "        clip = frames[start: start + window_size]\n",
    "        clips.append(clip)\n",
    "        indices.append(start)\n",
    "    return clips, indices\n",
    "\n",
    "# --- Load the trained model ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.load(\"collision_model_SlowR50.pth\", weights_only=False)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# --- Parameters ---\n",
    "test_folder = \"test\"    # folder with test videos (assumed to be .mp4)\n",
    "fps = 30                # frames per second (adjust if needed)\n",
    "window_size = 16        # fixed number of frames per clip\n",
    "stride = 8          # sliding window stride\n",
    "threshold = 0.9      # threshold to determine collision (target = 1)\n",
    "\n",
    "def save_resized_video(frames, output_path, fps=30, frame_size=(355, 200)):\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, frame_size)\n",
    "    for frame in frames:\n",
    "        out.write(frame)\n",
    "    out.release()\n",
    "\n",
    "\n",
    "\n",
    "# --- Process each test video ---\n",
    "for video_file in os.listdir(test_folder):\n",
    "    if video_file.endswith(\".mp4\"):\n",
    "        video_path = os.path.join(test_folder, video_file)\n",
    "        print(f\"\\nProcessing video: {video_file}\")\n",
    "        \n",
    "        # Read the video frames using OpenCV\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            success, frame = vidcap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            # Resize frames to the expected size (should match training resize)\n",
    "            frame = cv2.resize(frame, (355, 200))\n",
    "            frames.append(frame)\n",
    "        vidcap.release()\n",
    "        resized_output_path = os.path.join(\"resized_videos\", video_file)\n",
    "        os.makedirs(\"resized_videos\", exist_ok=True)\n",
    "        save_resized_video(frames, resized_output_path, fps=fps, frame_size=(355, 200))\n",
    "        \n",
    "        if len(frames) < window_size:\n",
    "            print(f\"Video {video_file} too short (only {len(frames)} frames). Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Extract sliding window clips and record the starting indices\n",
    "        clips, indices = sliding_window_clips(frames, window_size, stride)\n",
    "        \n",
    "        best_prob = 0.0\n",
    "        best_index = None\n",
    "        # Evaluate each clip\n",
    "        probabilities = []\n",
    "        for clip, idx in zip(clips, indices):\n",
    "            clip_tensor = test_transform_clip(clip)\n",
    "            clip_tensor = clip_tensor.permute(1, 0, 2, 3).unsqueeze(0).float().to(device)\n",
    "            with torch.no_grad():\n",
    "                output = model(clip_tensor)\n",
    "                probAvg = torch.sigmoid(output).max().item()\n",
    "                probabilities.append(probAvg)\n",
    "                if probAvg > best_prob:\n",
    "                    best_prob = probAvg\n",
    "                    best_index = idx\n",
    "        # Aggregate predictions (e.g., average)\n",
    "\n",
    "        avg_prob = np.mean(probabilities)\n",
    "        predicted_target = 1 if avg_prob >= threshold else 0\n",
    "        # Determine predicted target using a threshold\n",
    "        #predicted_target = 1 if best_prob >= threshold else 0\n",
    "        # Convert best_index to time in milliseconds: (frame index / fps) * 1000\n",
    "        if best_index is not None:\n",
    "            predicted_time_ms = (best_index / fps) * 1000\n",
    "        else:\n",
    "            predicted_time_ms = None\n",
    "        print(f\"Average Probability: {avg_prob:.2f} max: {best_prob:.2f}\")\n",
    "        if avg_prob>0.8:\n",
    "            print(f\"Predicted Target: {predicted_target} (Probability: {best_prob:.2f})\")\n",
    "            if predicted_time_ms is not None:\n",
    "                print(f\"Predicted Collision Time: {predicted_time_ms:.0f} ms\")\n",
    "        else:\n",
    "            print(\"No valid clip found for prediction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- run the cell to make test resized videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder = \"test\"    # folder with test videos (assumed to be .mp4)\n",
    "fps = 30                # frames per second (adjust if needed)\n",
    "window_size = 16        # fixed number of frames per clip\n",
    "stride = 4              # sliding window stride\n",
    "threshold = 0.6         # threshold to determine collision (target = 1)\n",
    "\n",
    "\n",
    "for video_file in os.listdir(test_folder):\n",
    "    if video_file.endswith(\".mp4\"):\n",
    "        video_path = os.path.join(test_folder, video_file)\n",
    "        #print(f\"\\nProcessing video: {video_file}\")\n",
    "        \n",
    "        # Read the video frames using OpenCV\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            success, frame = vidcap.read()\n",
    "            if not success:\n",
    "                break\n",
    "            # Resize frames to the expected size (should match training resize)\n",
    "            frame = cv2.resize(frame, (355, 200))\n",
    "            frames.append(frame)\n",
    "        vidcap.release()\n",
    "        resized_output_path = os.path.join(\"resized_videos\", video_file)\n",
    "        os.makedirs(\"resized_videos\", exist_ok=True)\n",
    "        save_resized_video(frames, resized_output_path, fps=fps, frame_size=(355, 200))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
