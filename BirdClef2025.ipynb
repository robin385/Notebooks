{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport torch\nimport torchaudio\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport librosa\nimport torch.nn.functional as F\n# Check if GPU is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n# Configuration settings\nclass Config:\n    # Data paths (update these if running in a different environment or directory structure)\n    data_dir = \"\"   # base directory for the competition data\n    train_metadata = os.path.join(data_dir, \"/kaggle/input/birdclef-2025/train.csv\")\n    train_audio_dir = os.path.join(data_dir, \"/kaggle/input/birdclef-2025/train_audio\")      # assuming train audio files are in train_audio/\n    train_soundscapes_dir = os.path.join(data_dir, \"/kaggle/input/birdclef-2025/train_soundscapes\")      # assuming train audio files are in train_audio/\n    test_audio_dir = os.path.join(data_dir, \"/kaggle/input/birdclef-2025/test_soundscapes\")  # assuming test soundscape files in test_soundscapes/\n    sample_submission = os.path.join(data_dir, \"/kaggle/input/birdclef-2025/sample_submission.csv\")\n    \n    # Audio processing parameters\n    sample_rate = 32000       # target sampling rate for audio\n    clip_duration = 5.0       # duration (in seconds) of each audio clip or segment\n    n_mels = 128              # number of mel frequency bins for spectrogram\n    n_fft = 1024              # FFT window size \n    hop_length = 500          # hop length for STFT (controls time resolution)\n    fmin = 40                 # min frequency for mel filter (in Hz)\n    fmax = 15000               # max frequency for mel filter (if None, use sr/2)\n    use_soundscape_noise_addition=True\n    use_wave_augment=True\n    # Data augmentation flags\n    use_spec_augment = True   # whether to apply SpecAugment (time/freq masking) on training spectrograms\n    use_mixup = True          # whether to apply Mixup augmentation during training\n    \n    # SpecAugment parameters\n    time_mask_param = 50      # max width of time masking (in time frames)\n    freq_mask_param = 15      # max width of frequency masking (in mel bins)\n    \n    # Training hyperparameters\n    batch_size = 32\n    num_epochs = 240\n    learning_rate = 1e-3\n    weight_decay = 1e-5       # weight decay for optimizer (regularization)\n    val_split = 0.1           # fraction of training data to use as validation\n    n_splits = 5 # Or your desired number of folds\n    \ncfg = Config()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.638728Z","iopub.execute_input":"2025-05-19T12:17:02.639020Z","iopub.status.idle":"2025-05-19T12:17:02.646150Z","shell.execute_reply.started":"2025-05-19T12:17:02.638994Z","shell.execute_reply":"2025-05-19T12:17:02.645364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load training metadata\ntrain_df = pd.read_csv(cfg.train_metadata)\nsub_df = pd.read_csv(cfg.sample_submission)\nspecies_columns = sub_df.columns[1:]   # all columns except 'row_id'\n\n# 2. Build a mapping from species code to integer index\nlabel_to_index = {species: i for i, species in enumerate(species_columns)}\n\n# 3. Map primary_label → target index\ntrain_df['target'] = train_df['primary_label'].map(label_to_index)\n\n# 4. Drop any rows where the label wasn’t found (just in case)\ntrain_df = train_df[train_df['target'].notna()].reset_index(drop=True)\ntrain_df['target'] = train_df['target'].astype(int)\n\n# 5. Quick sanity check\nprint(\"Columns now:\", train_df.columns.tolist())\nprint(\"Sample targets:\", train_df[['primary_label','target']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.647290Z","iopub.execute_input":"2025-05-19T12:17:02.647522Z","iopub.status.idle":"2025-05-19T12:17:02.784394Z","shell.execute_reply.started":"2025-05-19T12:17:02.647501Z","shell.execute_reply":"2025-05-19T12:17:02.783629Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Amphibian and Wildlife Sound Classification\n\nThis notebook implements a solution for the El Silencio Natural Reserve acoustic species identification competition.","metadata":{}},{"cell_type":"code","source":"# Define audio length in samples for 5 seconds\ntarget_samples = int(cfg.sample_rate * cfg.clip_duration)\n\n# Initialize mel spectrogram transformer and amplitude-to-dB converter from torchaudio\nmel_transform = torchaudio.transforms.MelSpectrogram(\n    sample_rate=cfg.sample_rate, n_fft=cfg.n_fft, hop_length=cfg.hop_length, \n    n_mels=cfg.n_mels, f_min=cfg.fmin, f_max=cfg.fmax\n)\namp_to_db_transform = torchaudio.transforms.AmplitudeToDB(stype=\"power\", top_db=80)\n\ndef load_audio_to_mel(file_path):\n    \"\"\"Load an audio file, crop/pad to cfg.clip_duration, and convert to a normalized log-mel spectrogram tensor.\"\"\"\n    # Load audio\n    waveform, sr = torchaudio.load(file_path)\n    # If stereo, convert to mono\n    if waveform.shape[0] > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)\n    # Resample if needed\n    if sr != cfg.sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, cfg.sample_rate)\n        waveform = resampler(waveform)\n        sr = cfg.sample_rate\n    # Ensure waveform length is target_samples (pad or crop)\n    length = waveform.shape[1]\n    if length > target_samples:\n        # center crop\n        start = (length - target_samples) // 2\n        waveform = waveform[:, start:start + target_samples]\n    elif length < target_samples:\n        # pad with zeros to both ends to center the waveform in 5s\n        pad_total = target_samples - length\n        pad_left = pad_total // 2\n        pad_right = pad_total - pad_left\n        waveform = torch.nn.functional.pad(waveform, (pad_left, pad_right))\n        length = waveform.shape[1]  # should now equal target_samples\n    # Compute mel spectrogram (power)\n    mel_spec = mel_transform(waveform)  # shape: [1, n_mels, time_frames]\n    # Convert to log-scale (dB)\n    mel_spec_db = amp_to_db_transform(mel_spec)  # shape: [1, n_mels, time_frames]\n    # Normalize dB values to [0, 1]\n    mel_spec_db = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-6)\n    # At this point, mel_spec_db is a 1xN_melsxT tensor ready for model input\n    return mel_spec_db\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.785912Z","iopub.execute_input":"2025-05-19T12:17:02.786227Z","iopub.status.idle":"2025-05-19T12:17:02.794714Z","shell.execute_reply.started":"2025-05-19T12:17:02.786206Z","shell.execute_reply":"2025-05-19T12:17:02.793892Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading and Preparation","metadata":{}},{"cell_type":"code","source":"# Load the training metadata\ntrain_df = pd.read_csv('/kaggle/input/birdclef-2025/train.csv')\nprint(f\"Training data shape: {train_df.shape}\")\n\n# Load taxonomy information\ntaxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\nprint(f\"Taxonomy data shape: {taxonomy_df.shape}\")\n\n# Display a few samples from the training data\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.795486Z","iopub.execute_input":"2025-05-19T12:17:02.795836Z","iopub.status.idle":"2025-05-19T12:17:02.907321Z","shell.execute_reply.started":"2025-05-19T12:17:02.795816Z","shell.execute_reply":"2025-05-19T12:17:02.906729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check for missing values\nprint(\"Missing values in training data:\")\nprint(train_df.isnull().sum())\n\n# Get unique species in the training data\nunique_species = train_df['primary_label'].unique()\nprint(f\"Number of unique species in training data: {len(unique_species)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.908923Z","iopub.execute_input":"2025-05-19T12:17:02.909569Z","iopub.status.idle":"2025-05-19T12:17:02.928956Z","shell.execute_reply.started":"2025-05-19T12:17:02.909543Z","shell.execute_reply":"2025-05-19T12:17:02.928251Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Audio Feature Extraction\n\nWe'll extract mel-spectrogram features from the audio files.","metadata":{}},{"cell_type":"code","source":"def load_audio_file(file_path, sr=32000, duration=5.0):\n    \"\"\"Load an audio file and return the waveform.\"\"\"\n    try:\n        y, sp = librosa.load(file_path, sr=sr, duration=duration, res_type='kaiser_fast')\n        # Pad if audio is shorter than expected duration\n        if len(y) < sr * duration:\n            y = np.pad(y, (0, int(sr * duration) - len(y)))\n        return y,sp\n    except Exception as e:\n        print(f\"Error loading {file_path}: {e}\")\n        return np.zeros(int(sr * duration))  # Return silence if there's an error\n\ndef extract_melspectrogram(y, sr=32000, n_mels=128, n_fft=2048, hop_length=512):\n    \"\"\"Extract mel-spectrogram from an audio waveform.\"\"\"\n    try:\n        melspec = librosa.feature.melspectrogram(\n            y=y, sr=sr, n_mels=n_mels, n_fft=n_fft, hop_length=hop_length\n        )\n        melspec_db = librosa.power_to_db(melspec, ref=np.max)\n        return melspec_db\n    except Exception as e:\n        print(f\"Error extracting melspectrogram: {e}\")\n        return np.zeros((n_mels, int(1 + (len(y) - n_fft) / hop_length)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.929711Z","iopub.execute_input":"2025-05-19T12:17:02.929939Z","iopub.status.idle":"2025-05-19T12:17:02.943148Z","shell.execute_reply.started":"2025-05-19T12:17:02.929924Z","shell.execute_reply":"2025-05-19T12:17:02.942453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to process training data\ndef prepare_training_data(df, max_samples_per_species=50):\n    X = []\n    y = []\n    species_labels = []\n    species_counts = df['primary_label'].value_counts()\n    \n    # Create a dictionary to keep track of samples per species\n    samples_per_species = {}\n    \n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing training data\"):\n        species = row['primary_label']\n        \n        # Skip if we already have enough samples for this species\n        if species in samples_per_species and samples_per_species[species] >= max_samples_per_species:\n            continue\n            \n        # Initialize counter for this species if not already present\n        if species not in samples_per_species:\n            samples_per_species[species] = 0\n        \n        file_path = f\"train_audio/{row['filename']}\"\n        \n        # Load audio and extract features\n        try:\n            y_audio = load_audio_file(file_path)\n            melspec = extract_melspectrogram(y_audio)\n            \n            X.append(melspec)\n            y.append(species)\n            \n            if species not in species_labels:\n                species_labels.append(species)\n                \n            # Increment counter for this species\n            samples_per_species[species] += 1\n        except Exception as e:\n            print(f\"Error processing {file_path}: {e}\")\n    \n    # Convert species labels to one-hot encoding\n    species_to_idx = {species: idx for idx, species in enumerate(species_labels)}\n    y_encoded = np.zeros((len(y), len(species_labels)))\n    for i, species in enumerate(y):\n        y_encoded[i, species_to_idx[species]] = 1.0\n    \n    # Convert to numpy arrays\n    X = np.array(X)\n    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)  # Add channel dimension for CNN\n    \n    return X, y_encoded, species_labels, species_to_idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.943776Z","iopub.execute_input":"2025-05-19T12:17:02.943981Z","iopub.status.idle":"2025-05-19T12:17:02.957868Z","shell.execute_reply.started":"2025-05-19T12:17:02.943967Z","shell.execute_reply":"2025-05-19T12:17:02.957210Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\nimport random\nimport os\n\ndef load_random_noise_mel():\n    \"\"\"\n    Pick a random .ogg file from cfg.test_audio_dir,\n    load it and convert to a normalized log-mel spectrogram.\n    \"\"\"\n    # find all .ogg files\n    files = glob.glob(os.path.join(cfg.train_soundscapes_dir, '*.ogg'))\n    if not files:\n        raise FileNotFoundError(f\"No .ogg files found in {cfg.train_soundscapes_dir}\")\n    \n    # pick one at random\n    fname = random.choice(files)\n    print(\"SOUNDSCAPES:\"+fname)\n    # load & preprocess\n    mel = load_audio_to_mel(fname)\n    return mel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.958636Z","iopub.execute_input":"2025-05-19T12:17:02.958839Z","iopub.status.idle":"2025-05-19T12:17:02.974850Z","shell.execute_reply.started":"2025-05-19T12:17:02.958825Z","shell.execute_reply":"2025-05-19T12:17:02.974055Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport random\nimport librosa\n\ndef augment_waveform(waveform: torch.Tensor, sample_rate: int) -> torch.Tensor:\n    \"\"\"\n    waveform: [1, n_samples] torch.Tensor on CPU\n    sample_rate: int, e.g. 32000\n    Returns: augmented waveform [1, *] torch.Tensor\n    \"\"\"\n    # to numpy 1D, ensure it's float32 from the start\n    w = waveform.squeeze(0).cpu().numpy().astype(np.float32)\n\n    # 1) Gaussian noise\n    if random.random() < 0.5:\n        amp = np.random.uniform(0.001, 0.015)\n        noise = np.random.randn(len(w)).astype(np.float32) * amp # Ensure noise is float32\n        w = w + noise\n\n    # 2) Time-stretch\n    if random.random() < 0.5:\n        rate = np.random.uniform(0.8, 1.25)\n        # librosa.effects.time_stretch preserves dtype of input if it's float32 or float64\n        w = librosa.effects.time_stretch(w, rate=rate)\n\n    # 3) Pitch-shift (sr and n_steps as keywords)\n    if random.random() < 0.5:\n        n_steps = np.random.uniform(-4, 4)  # fractional semitones\n        # librosa.effects.pitch_shift preserves dtype\n        w = librosa.effects.pitch_shift(w, sr=sample_rate, n_steps=n_steps)\n\n    # 4) Time-shift (circular roll ±10% length)\n    if random.random() < 0.5:\n        max_shift = int(0.1 * len(w))\n        shift = np.random.randint(-max_shift, max_shift)\n        w = np.roll(w, shift)\n\n    # Ensure w is float32 before converting back to tensor\n    w = w.astype(np.float32)\n    \n    # back to torch [1, n_samples]\n    w_tensor = torch.from_numpy(w).unsqueeze(0)\n    return w_tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.975635Z","iopub.execute_input":"2025-05-19T12:17:02.975883Z","iopub.status.idle":"2025-05-19T12:17:02.988956Z","shell.execute_reply.started":"2025-05-19T12:17:02.975862Z","shell.execute_reply":"2025-05-19T12:17:02.988244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Create the 'target' column by mapping primary_label to class indices\nsub_df = pd.read_csv(cfg.sample_submission)\nspecies_columns = sub_df.columns[1:]   # all species columns\nlabel_to_index = {species: i for i, species in enumerate(species_columns)}\n\ntrain_df['target'] = train_df['primary_label'].map(label_to_index)\ntrain_df = train_df[train_df['target'].notna()].reset_index(drop=True)\ntrain_df['target'] = train_df['target'].astype(int)\nprint(\"Columns now:\", train_df.columns.tolist())\n\n# 2. Define SpecAugment transforms (to be used only on training data)\ntime_masker = torchaudio.transforms.TimeMasking(time_mask_param=cfg.time_mask_param)\nfreq_masker = torchaudio.transforms.FrequencyMasking(freq_mask_param=cfg.freq_mask_param)\n\n# assume augment_waveform(waveform: Tensor, sr: int) is defined elsewhere\n# assume mel_transform and amp_to_db_transform are defined\n# assume freq_masker and time_masker are defined\n# assume target_samples (in samples) and cfg.sample_rate, cfg.use_wave_augment, cfg.use_spec_augment are available\n\nclass BirdClefDataset(Dataset):\n    def __init__(self, dataframe, audio_dir, augment=False):\n        \"\"\"\n        dataframe: DataFrame with 'filename' and 'target' columns.\n        audio_dir: Directory where audio files are stored.\n        augment: whether to apply waveform+SpecAugment.\n        \"\"\"\n        self.data      = dataframe\n        self.audio_dir = audio_dir\n        self.augment   = augment\n        # It's good practice to define fixed_frames once, perhaps based on a dummy input\n        # to ensure all spectrograms have the same time dimension before batching.\n        dummy_waveform = torch.zeros(1, int(cfg.sample_rate * cfg.clip_duration)) # Ensure this matches target_samples logic\n        with torch.no_grad():\n            mel_dummy = mel_transform(dummy_waveform) # [1, n_mels, T]\n        self.fixed_frames = mel_dummy.shape[-1]\n\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        fp = os.path.join(self.audio_dir, row['filename'])\n        fp = fp.strip() # Ensure no leading/trailing whitespace in filepath\n\n        # 1) Load & mono-mix & resample\n        try:\n            waveform, sr = torchaudio.load(fp)\n        except Exception as e:\n            print(f\"Error loading audio {fp}: {e}\")\n            # Return a dummy tensor or handle appropriately\n            waveform = torch.zeros(1, int(cfg.sample_rate * cfg.clip_duration))\n            sr = cfg.sample_rate\n\n        if waveform.shape[0] > 1:\n            waveform = waveform.mean(0, keepdim=True)\n        if sr != cfg.sample_rate:\n            resampler = torchaudio.transforms.Resample(sr, cfg.sample_rate)\n            waveform = resampler(waveform)\n            sr = cfg.sample_rate # Update sr after resampling\n\n        # Ensure waveform is float for augmentations and model input\n        waveform = waveform.float()\n\n        # 2) Enforce fixed length (target_samples) BEFORE augmentation if augmentations change length\n        current_length = waveform.shape[1]\n        if current_length > target_samples:\n            waveform = waveform[:, :target_samples]\n        elif current_length < target_samples:\n            pad_amt = target_samples - current_length\n            waveform = F.pad(waveform, (0, pad_amt))\n\n\n        # 3) Waveform‐level augmentations\n        if self.augment and cfg.use_wave_augment:\n            waveform = augment_waveform(waveform, sr) # augment_waveform should handle [1, n_samples]\n\n        # Re-ensure fixed length AFTER augmentation if augmentations might change length\n        # This step might be redundant if augment_waveform preserves length or if padding is done carefully after it.\n        current_length = waveform.shape[1]\n        if current_length > target_samples:\n            # This could happen if time-stretching makes it longer\n            start = (current_length - target_samples) // 2 # Center crop\n            waveform = waveform[:, start:start + target_samples]\n        elif current_length < target_samples:\n            # This could happen if time-stretching makes it shorter\n            pad_amt = target_samples - current_length\n            waveform = F.pad(waveform, (0, pad_amt))\n\n\n        # 4) Convert to mel, to dB, normalize\n        # Ensure waveform is on CPU for torchaudio transforms if not already\n        #mel = mel_transform(waveform.cpu())            # [1, n_mels, T]\n        mel = mel_transform(waveform.cpu().float())\n        mel = amp_to_db_transform(mel)\n        # Normalize dB values to [0, 1]\n        mel_min = mel.min()\n        mel_max = mel.max()\n        if mel_max - mel_min == 0: # Avoid division by zero if mel is flat (e.g., silence)\n             mel = torch.zeros_like(mel)\n        else:\n            mel = (mel - mel_min) / (mel_max - mel_min + 1e-6)\n\n\n        # Ensure fixed time frames for mel spectrogram\n        T = mel.size(-1)\n        if T > self.fixed_frames:\n            mel = mel[..., :self.fixed_frames]\n        elif T < self.fixed_frames:\n            mel = F.pad(mel, (0, self.fixed_frames - T))\n\n\n        # 5) SpecAugment on mel\n        if self.augment and cfg.use_spec_augment:\n            if random.random() < 0.5:\n                mel = freq_masker(mel)\n            if random.random() < 0.5:\n                mel = time_masker(mel)\n\n        label = row['target']\n        return mel, label\n\nfrom sklearn.model_selection import KFold # <-- Add this line\n# K-Fold Cross Validation Setup\nkf = KFold(n_splits=cfg.n_splits, shuffle=True, random_state=42) # random_state for reproducibility\n\n# We will create DataLoaders inside the training loop for each fold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:02.989715Z","iopub.execute_input":"2025-05-19T12:17:02.989909Z","iopub.status.idle":"2025-05-19T12:17:03.024043Z","shell.execute_reply.started":"2025-05-19T12:17:02.989895Z","shell.execute_reply":"2025-05-19T12:17:03.023510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport timm\n\n# Create EfficientNet-B0 model\nmodel = timm.create_model('efficientnetv2_rw_m.agc_in1k', pretrained=True, in_chans=1, num_classes=len(species_columns))\n#if torch.cuda.device_count() > 1:\n#    print(f\"Using {torch.cuda.device_count()} GPUs\")\n#    model = nn.DataParallel(model)\nmodel.to(device)\nprint(model.architecture if hasattr(model, 'architecture') else type(model))\n\n# Define loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:03.025999Z","iopub.execute_input":"2025-05-19T12:17:03.026198Z","iopub.status.idle":"2025-05-19T12:17:04.880584Z","shell.execute_reply.started":"2025-05-19T12:17:03.026183Z","shell.execute_reply":"2025-05-19T12:17:04.879784Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Definition\n\nWe'll use a Convolutional Neural Network (CNN) to classify the spectrograms.","metadata":{}},{"cell_type":"code","source":"# Lists to store metrics from each fold\nfold_train_losses = []\nfold_val_losses = []\nfold_val_accuracies = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n    print(f\"--- Fold {fold+1}/{cfg.n_splits} ---\")\n\n    # --- Data for current fold ---\n    train_data_fold = train_df.iloc[train_idx].reset_index(drop=True)\n    val_data_fold = train_df.iloc[val_idx].reset_index(drop=True)\n\n    print(f\"Training samples for fold {fold+1}: {len(train_data_fold)}, Validation samples: {len(val_data_fold)}\")\n\n    train_dataset_fold = BirdClefDataset(train_data_fold, audio_dir=cfg.train_audio_dir, augment=True)\n    val_dataset_fold = BirdClefDataset(val_data_fold, audio_dir=cfg.train_audio_dir, augment=False)\n\n    train_loader_fold = DataLoader(train_dataset_fold, batch_size=cfg.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader_fold = DataLoader(val_dataset_fold, batch_size=cfg.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n\n    # --- Re-initialize model and optimizer for each fold ---\n    # Create EfficientNet-B0 model (or your chosen model)\n    model = timm.create_model('efficientnetv2_rw_m.agc_in1k', pretrained=True, in_chans=1, num_classes=len(species_columns))\n    model.to(device)\n    # if torch.cuda.device_count() > 1:\n    # print(f\"Using {torch.cuda.device_count()} GPUs\")\n    # model = nn.DataParallel(model)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=cfg.weight_decay)\n    # Optional: Re-initialize learning rate scheduler here if you are using one\n\n    # --- Training loop for the current fold ---\n    best_val_loss_fold = float('inf')\n    best_model_state_fold = None # Store best model for this fold\n    # best_path_fold = f\\\"/kaggle/working/best_model_fold_{fold+1}.pth\\\" # Save best model per fold\n\n    epochs_no_improve = 0\n    patience = 5 # You can also put this in cfg\n\n    for epoch in range(1, cfg.num_epochs + 1):\n        model.train()\n        running_loss = 0.0\n\n        for batch_idx, (inputs, targets) in enumerate(train_loader_fold):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            if cfg.use_mixup:\n                lam = np.random.beta(0.2, 0.2)\n                indices = torch.randperm(inputs.size(0)).to(device) # Ensure indices are on the same device\n                inputs_shuffled = inputs[indices]\n                targets_shuffled = targets[indices]\n                \n                inputs_mixed = lam * inputs + (1 - lam) * inputs_shuffled\n                outputs = model(inputs_mixed)\n                loss = lam * criterion(outputs, targets) + (1 - lam) * criterion(outputs, targets_shuffled)\n            else:\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n\n        avg_train_loss = running_loss / len(train_loader_fold)\n\n        # Validation for the current fold\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, targets in val_loader_fold:\n                inputs, targets = inputs.to(device), targets.to(device)\n                outputs = model(inputs)\n                loss_val_batch = criterion(outputs, targets) # Use a different variable name\n                val_loss += loss_val_batch.item()\n                _, predicted = torch.max(outputs.data, 1)\n                total += targets.size(0)\n                correct += (predicted == targets).sum().item()\n        \n        avg_val_loss = val_loss / len(val_loader_fold)\n        val_acc = correct / total\n\n        improved = False\n        if avg_val_loss < best_val_loss_fold:\n            best_val_loss_fold = avg_val_loss\n            # torch.save(model.state_dict(), best_path_fold) # Optionally save best model for this fold\n            best_model_state_fold = model.state_dict().copy() # Important to copy\n            epochs_no_improve = 0\n            improved = True\n        else:\n            epochs_no_improve += 1\n\n        print(f\"Fold {fold+1}, Epoch {epoch}: Train Loss = {avg_train_loss:.4f},\"\n              f\"Val Loss = {avg_val_loss:.4f}, Val Acc = {val_acc:.4f}\"\n              f\"{'  <-- improved' if improved else ''}\")\n\n        if epochs_no_improve >= patience:\n            print(f\"No improvement for {patience} epochs in fold {fold+1}. Early stopping at epoch {epoch}.\")\n            break\n    \n    # After all epochs for the current fold are done\n    fold_train_losses.append(avg_train_loss) # Store last epoch's train loss, or best, as you prefer\n    fold_val_losses.append(best_val_loss_fold) # Store best validation loss for this fold\n    fold_val_accuracies.append(val_acc) # Store validation accuracy corresponding to best_val_loss_fold (or last epoch)\n\n    # Optional: Load the best model state for this fold if you plan to use it immediately\n    # if best_model_state_fold:\n    #     model.load_state_dict(best_model_state_fold)\n    #     print(f\"Loaded best model weights for fold {fold+1} with Val Loss = {best_val_loss_fold:.4f}\")\n\n# --- After all folds are completed ---\nprint(\" K-Fold Cross-Validation Summary\")\nprint(f\"Average Training Loss across folds: {np.mean(fold_train_losses):.4f}\")\nprint(f\"Average Validation Loss across folds: {np.mean(fold_val_losses):.4f} (Std: {np.std(fold_val_losses):.4f})\")\nprint(f\"Average Validation Accuracy across folds: {np.mean(fold_val_accuracies):.4f} (Std: {np.std(fold_val_accuracies):.4f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:17:04.881731Z","iopub.execute_input":"2025-05-19T12:17:04.881986Z","execution_failed":"2025-05-19T12:38:55.194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load best model weights before inference (if we saved any)\nif best_model_state is not None:\n    model.load_state_dict(best_model_state)\nmodel.eval()\n\n# Prepare submission dataframe for output\nsubmission_df = sub_df.copy()\n\n# Group row_ids by soundscape file\nsubmission_df['audio_id'] = submission_df['row_id'].apply(lambda x: x.rsplit('_', 1)[0])  # everything except last underscore part is audio id\nunique_files = submission_df['audio_id'].unique()\n\n# Iterate over each unique soundscape file\nfor audio_id in unique_files:\n    audio_path = os.path.join(cfg.test_audio_dir, audio_id + \".ogg\")\n    if not os.path.exists(audio_path):\n        print(f\"Warning: audio file {audio_path} not found. Skipping.\")\n        continue\n    # Load full audio\n    waveform, sr = torchaudio.load(audio_path)\n    if waveform.shape[0] > 1:\n        waveform = waveform.mean(dim=0, keepdim=True)  # convert to mono\n    if sr != cfg.sample_rate:\n        resampler = torchaudio.transforms.Resample(sr, cfg.sample_rate)\n        waveform = resampler(waveform)\n        sr = cfg.sample_rate\n    waveform = waveform.squeeze(0)  # shape: [samples]\n    total_len = waveform.shape[0]\n    # Get all rows for this audio file\n    file_rows = submission_df[submission_df['audio_id'] == audio_id]\n    for idx, row in file_rows.iterrows():\n        row_id = row['row_id']\n        # Parse end_time from row_id (everything after last underscore)\n        end_time = int(row_id.split('_')[-1])\n        start_time = end_time - 5\n        # Convert times to sample indices\n        start_sample = int(start_time * cfg.sample_rate)\n        end_sample = int(end_time * cfg.sample_rate)\n        if start_sample < 0: start_sample = 0\n        if end_sample > total_len: end_sample = total_len\n        segment_waveform = waveform[start_sample:end_sample]\n        # Pad or crop segment_waveform to exactly 5 seconds in length\n        segment_len = segment_waveform.shape[0]\n        if segment_len < target_samples:\n            pad_needed = target_samples - segment_len\n            segment_waveform = torch.nn.functional.pad(segment_waveform, (0, pad_needed))\n        elif segment_len > target_samples:\n            segment_waveform = segment_waveform[:target_samples]\n        # Now compute spectrogram for this segment\n        segment_waveform = segment_waveform.unsqueeze(0)  # add channel dim\n        mel_spec = mel_transform(segment_waveform)          # [1, n_mels, time]\n        mel_spec_db = amp_to_db_transform(mel_spec)        # [1, n_mels, time] in dB\n        # Normalize to [0,1] (use same method as in training)\n        mel_spec_db = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-6)\n        mel_spec_db = mel_spec_db.to(device)\n        # Predict with model\n        with torch.no_grad():\n            output = model(mel_spec_db)\n            probs = torch.softmax(output, dim=1)  # get probabilities over classes\n            probs = probs.squeeze().cpu().numpy()  # shape: (206,)\n        # Fill the probabilities into the submission dataframe for this row\n        submission_df.loc[idx, species_columns] = probs\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-19T12:38:55.195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop the helper column\nsubmission_df = submission_df.drop(columns=['audio_id'])\n# Save to CSV\nsubmission_df.to_csv(\"/kaggle/input/birdclef-2025/sample_submission.csv\", index=False)\nprint(\"Submission file saved with shape:\", submission_df.shape)\nprint(submission_df.head(10))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-19T12:38:55.195Z"}},"outputs":[],"execution_count":null}]}